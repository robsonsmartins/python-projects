{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DesafioIA-Serpro-PLN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgGtL3H78dvi",
        "colab_type": "text"
      },
      "source": [
        "# Desafio IA SERPRO 2020 -  PLN\n",
        "\n",
        "Robson de Sousa Martins\n",
        "________________________________________________________________________________________________________________________\n",
        "\n",
        "**Página no Desafio:** http://evalai.dev.serpro/web/challenges/challenge-page/15/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J27Mb2H8jDh",
        "colab_type": "text"
      },
      "source": [
        "## Bibliotecas Utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seyJjgCl8lEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from statistics import mean \n",
        "\n",
        "import spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1j-F6f2KCOO6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clssificadores\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.tree import ExtraTreeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB  \n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUTHKfSq8-bb",
        "colab_type": "text"
      },
      "source": [
        "## Download de Módulos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDThr3t19JCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "fb09e5af-4a93-48b8-af30-1030835ea976"
      },
      "source": [
        "!python -m spacy download pt\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pt_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: pt-core-news-sm\n",
            "  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=5eae05eba4fe239b27da6c18131fe33ea9c6bff99341b0848bb905fadfa2a40e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vx6s480l/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n",
            "Successfully built pt-core-news-sm\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('pt_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "You can now load the model via spacy.load('pt')\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUC3f4gz9KTb",
        "colab_type": "text"
      },
      "source": [
        "## Funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tBD15hsDUfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Determinando forma básica (lema) das palavras\n",
        "# Isso só vale pra verbos (\"colocar no infinitivo\")\n",
        "def lemmatizer(text):\n",
        "  sent = []\n",
        "  # uso o lemmatizer do spacy\n",
        "  doc = nlp(text)\n",
        "  for word in doc:\n",
        "    if word.pos_ == \"VERB\":\n",
        "      sent.append(word.lemma_)\n",
        "    else:\n",
        "      sent.append(word.orth_)\n",
        "  return \" \".join(sent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj7s_q31CFNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforma números em palavras por extenso\n",
        "# Isso acabou por ser relevante na base de dados trabalhada, e elevou a \n",
        "# performance dos modelos\n",
        "\n",
        "# Marcos Paulo Ferreira (Daemonio)\n",
        "# https://daemoniolabs.wordpress.com\n",
        "# Versão 1.1 by daemonio @Sat Dec 20 23:41:50 BRST 2014\n",
        "class dExtenso():\n",
        "    trioextenso=()\n",
        "    classextenso=()\n",
        " \n",
        "    def __init__(self):\n",
        "        self.trioextenso=(\n",
        "                     (\"dummy\",\"um\",\"dois\",\"três\",\"quatro\",\"cinco\",\"seis\",\"sete\",\n",
        "                     \"oito\",\"nove\"),\n",
        "                     (\"dez\",\"onze\",\"doze\",\"treze\",\"quatorze\",\"quinze\",\"dezesseis\",\n",
        "                     \"dezessete\",\"dezoito\",\"dezenove\"),\n",
        "                     (\"dummy\",\"dummy\",\"vinte\",\"trinta\",\"quarenta\",\"cinquenta\",\n",
        "                     \"sessenta\",\"setenta\",\"oitenta\",\"noventa\"),\n",
        "                     (\"dummy\",\"cento\",\"duzentos\",\"trezentos\",\"quatrocentos\",\n",
        "                     \"quinhentos\",\"seiscentos\",\"setecentos\",\"oitocentos\",\n",
        "                     \"novecentos\"))\n",
        "        self.classextenso=(\n",
        "                      \"dummy\",\"mil\",\"milh\",\"bilh\",\"trilh\",\"quatrilh\",\n",
        "                      \"quintilh\",\"sextilh\",\"septilh\",\"octilh\",\n",
        "                      \"nonilh\",\"decilh\",\"undecilh\",\"duodecilh\",\n",
        "                      \"tredecilh\",\"quatordecilh\",\"quindecilh\",\n",
        "                      \"sexdecilh\",\"setedecilh\",\"octodecilh\",\n",
        "                      \"novedecilh\",\"vigesilh\" )\n",
        "  \n",
        "    def escrever_trio_extenso(self, trio):\n",
        "        saida=[]\n",
        "        if trio == '100':\n",
        "            return 'cem'\n",
        "        elif trio == '000':\n",
        "            return 'zero'\n",
        "        else:\n",
        "            c, d, u = trio\n",
        "            c, d, u = int(c), int(d), int(u)\n",
        "            if c != 0:\n",
        "                saida.append(self.trioextenso[3][c])\n",
        "            if d == 1:\n",
        "                saida.append(self.trioextenso[1][u])\n",
        "            else:\n",
        "                if d != 0:\n",
        "                    saida.append(self.trioextenso[2][d])\n",
        "                if u != 0:\n",
        "                    saida.append(self.trioextenso[0][u])\n",
        "        return ' e '.join(saida)\n",
        "  \n",
        "    def nao_e_ultimo_trio(self, totalTrios, contador):\n",
        "        return contador < (totalTrios - 1)\n",
        "  \n",
        "    def trio_a_esquerda_eq_zero(self, trioLista, contador):\n",
        "        t = len(trioLista)-1\n",
        "        return trioLista[t-contador-1] == '000'\n",
        "  \n",
        "    def getExtenso(self, num, quebradelinhas=0):\n",
        "        # by Robson Martins\n",
        "        if num == \"r$\": # R$ xxxx,xx\n",
        "          return \"real\"\n",
        "        if num == \"us$\": # US$ xxxx,xx\n",
        "          return \"dólar\"\n",
        "        num = re.sub(r'[\\.]{1}','',num) # x.xxx.xxx\n",
        "        num = re.sub(r'[\\,]{1}[0-9]+','',num) # xxx,yyy\n",
        "        '''\n",
        "        Ordinais: aqui estou transformando ordinais em números cardinais por\n",
        "        extenso.\n",
        "        TODO: uma possível melhora é converter ordinais para extenso\n",
        "        '''\n",
        "        num = re.sub(r'([0-9]+)[oaªº°]{1}',r'\\1',num)\n",
        "        if not num.isnumeric():\n",
        "          return num\n",
        "        # / by Robson Martins\n",
        "        num = num.lstrip('0')\n",
        "        pad = 3 - len(num)%3\n",
        "        if pad < 3: num = '0'*pad + num\n",
        "        it = iter(num)\n",
        "        trioLista = [ ''.join([a,b,c]) for a, b, c in zip(it, it, it)]\n",
        "        if len(trioLista) > len(self.classextenso):\n",
        "            raise IndexError\n",
        "        contador=0\n",
        "        saida=''\n",
        "        extensofinal=''\n",
        "        for trio in reversed(trioLista):\n",
        "            trioInt=int(trio)\n",
        "            if trioInt > 0:\n",
        "                saida = self.escrever_trio_extenso(trio)\n",
        "                if contador > 0:\n",
        "                    saida = saida + ' ' + self.classextenso[contador]\n",
        "                if contador > 1:\n",
        "                    if trioInt > 1:\n",
        "                        saida = saida + 'ões'\n",
        "                    else:\n",
        "                        saida = saida + 'ão'\n",
        "                if quebradelinhas == 0:\n",
        "                    if self.nao_e_ultimo_trio(len(trioLista), contador):\n",
        "                        if self.trio_a_esquerda_eq_zero(trioLista, contador):\n",
        "                            saida = ' e ' + saida\n",
        "                        elif trioInt >= 100:\n",
        "                            saida = ', ' + saida\n",
        "                        else:\n",
        "                            saida = ' e ' + saida\n",
        "                else:\n",
        "                    saida = saida + '\\n'\n",
        "                extensofinal = saida + extensofinal\n",
        "            contador = contador + 1\n",
        "        return extensofinal.rstrip('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gx4OzlA9Pvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Realiza uma limpeza básica de um texto, preparando-o para classificação.\n",
        "def limpar_texto(texto):\n",
        "    # Converte para minúsculas\n",
        "    texto = texto.lower()\n",
        "    # Troca números para extenso\n",
        "    extenso = dExtenso()\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "    tokens = [extenso.getExtenso(palavra.strip()) for palavra in tokens]\n",
        "    texto = ' '.join(tokens)  \n",
        "    # Remove números, se restaram\n",
        "    texto = re.sub(r'[0-9]+',' ',texto)\n",
        "    # Remove pontuacao\n",
        "    texto = texto.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
        "    # Remove espacos extras\n",
        "    texto = re.sub(r'\\s+',' ',texto)\n",
        "    # Remove stopwords\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "    tokens = [palavra.strip() for palavra in tokens if palavra not in stopwords]\n",
        "    texto = ' '.join(tokens)  \n",
        "    # Lematiza palavras\n",
        "    texto = lemmatizer(texto)\n",
        "    # Remove acentos\n",
        "    texto = normalize('NFKD', texto).encode('ASCII', 'ignore').decode('ASCII')\n",
        "    # cria dict de palavras unicas\n",
        "    # remove palavras menores que 2 caracteres\n",
        "    tokens = tokenizer.tokenize(texto)\n",
        "    fdist = nltk.FreqDist(tokens)\n",
        "    tokens = [palavra.strip() for palavra, freq in fdist.items() if len(palavra) >= 2]\n",
        "    texto = ' '.join(tokens)  \n",
        "    return texto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qHDZXbKeqM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Treina um classificador, otimiza hiperparâmetros,\n",
        "# avalia performance e retorna métricas de desempenho\n",
        "def build(X,y,vec,est,grid):\n",
        "  est_name = est.__class__.__name__\n",
        "  vec_name = vec.__class__.__name__\n",
        "  print('Testando o classificador',est_name,'-',vec_name,'...')\n",
        "  # Massa de Dados\n",
        "  X_data = vec.transform(X.tolist()).toarray()\n",
        "  y_data = y.tolist()\n",
        "  # Otimiza modelos\n",
        "  # Uso F1 Macro como métrica\n",
        "  clf = GridSearchCV(est,grid,scoring='f1_macro',n_jobs=-1,cv=5,verbose=100)\n",
        "  clf.fit(X_data,y_data)\n",
        "  # Obtém as métricas de desempenho - o quanto nosso classificador acertou?\n",
        "  return clf.best_estimator_, est_name, vec_name, clf.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8rSJaEJ9eru",
        "colab_type": "text"
      },
      "source": [
        "## Inicialização"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoIfnUBJ9fh5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenizador: utilizado para separar uma frase em palavras\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# stopwords do português\n",
        "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
        "\n",
        "# lib spacy para fazer o lemmatizer\n",
        "nlp = spacy.load('pt')\n",
        "\n",
        "# Semente aleatória a ser usada ao longo desse notebook.\n",
        "# Procure manter sempre a mesma semente aleatória. Desse modo, poderá comparar a evolução entre diferentes técnicas\n",
        "random_state=660601\n",
        "\n",
        "# Nome do arquivo fornecido pelo desafio com os dados rotulados para treino\n",
        "nome_arquivo_com_rotulos_para_treino = 'treino.csv'\n",
        "\n",
        "# Nome do arquivo fornecido pelo desafio com os dados não rotulados, que deverão ser classificados pelo modelo construído aqui\n",
        "nome_arquivo_sem_rotulos = 'teste-sem-classe.csv'\n",
        "\n",
        "# Nome do arquivo que será criado com os rótulos gerados pelo classificador\n",
        "# Esse é o arquivo se será submetido à página do desafio\n",
        "nome_arquivo_rotulado_classificador = 'teste.csv'\n",
        "\n",
        "# Nome do arquivo com os dados rotulados para treino, após preprocessamento (gero isso pra verificar minha rotina de preprocessamento)\n",
        "nome_arquivo_com_rotulos_para_treino_preprocessado = 'treino-preprocessado.csv'\n",
        "\n",
        "# Nome do arquivo com os dados não rotulados, após preprocessamento (gero isso pra verificar minha rotina de preprocessamento)\n",
        "nome_arquivo_sem_rotulos_preprocessado = 'teste-sem-classe-preprocessado.csv'\n",
        "\n",
        "# Caminho do Google Drive onde os arquivos estão armazenados\n",
        "# Uso a integração do Google Colab com o Google Drive\n",
        "# Se for usar um Jupyter fora do Colab e os arquivos estiverem no mesmo nível do Notebook,\n",
        "# coloque path_meu_google_drive = ''\n",
        "path_meu_google_drive = '/content/drive/My Drive/Datasets/Desafio IA 2020 PLN/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UbtPDix9uUN",
        "colab_type": "text"
      },
      "source": [
        "## Carregando os dados rotulados\n",
        "\n",
        "Nessa etapa carregamos os dados rotulados fornecidos no desafio. Eles serão usados para treinamento de um classificador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZUW9EF99vas",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "abc94884-7ed9-4f5a-c93d-1da39c9abc5a"
      },
      "source": [
        "path = path_meu_google_drive + nome_arquivo_com_rotulos_para_treino\n",
        "df = pd.read_csv(path, index_col=None, engine='python', sep =',', encoding=\"utf-8\")\n",
        "print('Total de registros carregados:',len(df))\n",
        "\n",
        "# Exibe uma amostra dos dados\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de registros carregados: 1700\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>675</td>\n",
              "      <td>lula diz que senado tem maioridade para resolv...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1733</td>\n",
              "      <td>adolescente é morto por ouvir música alta nos ...</td>\n",
              "      <td>tristeza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1855</td>\n",
              "      <td>coreia do sul insinua que hackers ligados à co...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1144</td>\n",
              "      <td>mamãe foca dá selinho em filhote recém-nascido...</td>\n",
              "      <td>alegria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>462</td>\n",
              "      <td>adolescente de 15 anos que estava sumida é ach...</td>\n",
              "      <td>tristeza</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                              texto    classe\n",
              "0   675  lula diz que senado tem maioridade para resolv...    neutro\n",
              "1  1733  adolescente é morto por ouvir música alta nos ...  tristeza\n",
              "2  1855  coreia do sul insinua que hackers ligados à co...    neutro\n",
              "3  1144  mamãe foca dá selinho em filhote recém-nascido...   alegria\n",
              "4   462  adolescente de 15 anos que estava sumida é ach...  tristeza"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0navliO1-cJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "4f31cdf5-85bc-4230-b63c-b99806d0f856"
      },
      "source": [
        "# Distribuição das classes nos dados fornecidos\n",
        "df.groupby('classe').count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>classe</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>alegria</th>\n",
              "      <td>156</td>\n",
              "      <td>156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>desgosto</th>\n",
              "      <td>223</td>\n",
              "      <td>223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medo</th>\n",
              "      <td>189</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>neutro</th>\n",
              "      <td>461</td>\n",
              "      <td>461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>raiva</th>\n",
              "      <td>70</td>\n",
              "      <td>70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>surpresa</th>\n",
              "      <td>214</td>\n",
              "      <td>214</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tristeza</th>\n",
              "      <td>387</td>\n",
              "      <td>387</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           id  texto\n",
              "classe              \n",
              "alegria   156    156\n",
              "desgosto  223    223\n",
              "medo      189    189\n",
              "neutro    461    461\n",
              "raiva      70     70\n",
              "surpresa  214    214\n",
              "tristeza  387    387"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er-ZOCIu-hIC",
        "colab_type": "text"
      },
      "source": [
        "## Preparando os textos para classificação\n",
        "\n",
        "A preparação dos dados é uma das etapas mais importantes para se obter uma boa performance na classificação de textos, e pode significar a diferença entre o sucesso e o fracasso de um projeto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHqdmr9w-pd_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "43bacec1-0aad-4387-cbae-08bbe4e27eea"
      },
      "source": [
        "df['texto'] = df['texto'].apply(limpar_texto)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>675</td>\n",
              "      <td>lula dizer senado maioridade resolver problema...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1733</td>\n",
              "      <td>adolescente morto ouvir musica alta estados un...</td>\n",
              "      <td>tristeza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1855</td>\n",
              "      <td>coreia sul insinuar hackers ligar norte atacar...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1144</td>\n",
              "      <td>mamae foca dar selinho filhote recem nascido b...</td>\n",
              "      <td>alegria</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>462</td>\n",
              "      <td>adolescente quinze anos sumir achar morta mana...</td>\n",
              "      <td>tristeza</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                              texto    classe\n",
              "0   675  lula dizer senado maioridade resolver problema...    neutro\n",
              "1  1733  adolescente morto ouvir musica alta estados un...  tristeza\n",
              "2  1855  coreia sul insinuar hackers ligar norte atacar...    neutro\n",
              "3  1144  mamae foca dar selinho filhote recem nascido b...   alegria\n",
              "4   462  adolescente quinze anos sumir achar morta mana...  tristeza"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvSjtraIbj4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salva os registros de treino pre-processados\n",
        "path = path_meu_google_drive + nome_arquivo_com_rotulos_para_treino_preprocessado\n",
        "df.to_csv(path, index=False, encoding=\"utf-8\", columns=['id','texto','classe'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSD8F00f_HxF",
        "colab_type": "text"
      },
      "source": [
        "## Treinando e testando métodos de vetorização e classificador\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVtoYYXe_iFh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e91523af-0c78-427f-bb66-ee1da5c78f8d"
      },
      "source": [
        "# Vetorizadores\n",
        "\n",
        "countVectorizer = CountVectorizer()\n",
        "countVectorizer.fit_transform(df['texto'].tolist())\n",
        "\n",
        "tfidfVectorizer = TfidfVectorizer()\n",
        "tfidfVectorizer.fit_transform(df['texto'].tolist())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1700x8292 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 33660 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku5vf6uZg5oj",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "# Combinações de classificador/vetorizador\n",
        "# Vetorizador TfidfVectorizer\n",
        " \n",
        "estimators = [\n",
        "  {'est': MLPClassifier(), \n",
        "   'grid':{\n",
        "      #'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
        "      #'hidden_layer_sizes': [(50),(100),(200)],\n",
        "      'random_state': [random_state],\n",
        "      'max_iter':[200],\n",
        "      #'solver': ['lbfgs', 'sgd', 'adam'],\n",
        "      #'alpha': [1e-5,1e-4,1e-3,1e-2,1e-1,1.0],\n",
        "      #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
        "      },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': MultinomialNB(), \n",
        "   'grid':{\n",
        "#      'fit_prior' : {True},\n",
        "#      'alpha': [1e-3,1e-2,1e-1,1.0],\n",
        "#      'class_prior': [None],\n",
        "      },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': LogisticRegression(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "      'n_jobs':[-1],\n",
        "#      'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
        "#      'tol': [1e-4,1e-3,1e-2,1e-1,1.0],\n",
        "#      'C': [1e-1,1.0,2.0,3.0],\n",
        "#      'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "      },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': SGDClassifier(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "      'n_jobs':[-1],\n",
        "#      'alpha': [1e-5],\n",
        "#      'loss': ['hinge'],\n",
        "#      'penalty': ['elasticnet'],\n",
        "      },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': ExtraTreeClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': DecisionTreeClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': Perceptron(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': LinearSVC(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': RandomForestClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': ExtraTreesClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': GradientBoostingClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': RidgeClassifier(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': PassiveAggressiveClassifier(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "      'n_jobs':[-1],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': GaussianProcessClassifier(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'n_jobs':[-1],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': AdaBoostClassifier(), \n",
        "   'grid':{'random_state':[random_state]},\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': BaggingClassifier(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'n_jobs':[-1],\n",
        "      },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': BernoulliNB(), \n",
        "   'grid':{ },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': LinearDiscriminantAnalysis(), \n",
        "   'grid':{ },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': NearestCentroid(), \n",
        "   'grid':{ \n",
        "      'metric': ['euclidean', 'l2'], \n",
        "      'shrink_threshold': [None, 1e-1],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': QuadraticDiscriminantAnalysis(), \n",
        "   'grid':{ },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "  {'est': SVC(), \n",
        "   'grid':{\n",
        "      'random_state':[random_state],\n",
        "      'max_iter':[10000],\n",
        "    },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyEz0_iDVWQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combinações de classificador/vetorizador\n",
        "# Vetorizador TfidfVectorizer\n",
        "# Somente melhor classificador: NearestCentroid \n",
        "estimators = [\n",
        "  {'est': NearestCentroid(), \n",
        "   'grid':{ \n",
        "      'metric': ['euclidean', 'l2', 'manhattan'], \n",
        "      'shrink_threshold': [None, 1e-1],\n",
        "   },\n",
        "   'vec': tfidfVectorizer, 'est_name':'', 'vec_name':'', 'precision':0.0, 'recall':0.0, 'accuracy':0.0, 'f1':0.0},\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCE9x7sZK2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combinações de classificador/vetorizador\n",
        "# Vetorizador CountVectorizer\n",
        "estimators2 = []\n",
        "for estimator in estimators:\n",
        "  item2 = estimator.copy()\n",
        "  item2['vec'] = countVectorizer\n",
        "  estimators2.append(item2)\n",
        "\n",
        "estimators.extend(estimators2)   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qd2CN-ahOYtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "951b4b64-ab0c-461f-d2ed-248d3e46f8d2"
      },
      "source": [
        "# Treina/testa classificador/vetorizador\n",
        "for estimator in estimators:\n",
        "  estimator['est'], estimator['est_name'], estimator['vec_name'], estimator['f1'] = build(df['texto'],df['classe'],estimator['vec'],estimator['est'],estimator['grid'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testando o classificador NearestCentroid - TfidfVectorizer ...\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.5s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    0.8s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.1s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    1.8s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    2.1s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    2.4s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.8s\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.0s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    3.3s\n",
            "[Parallel(n_jobs=-1)]: Done  28 out of  30 | elapsed:    3.5s remaining:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.8s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    3.8s finished\n",
            "Testando o classificador NearestCentroid - CountVectorizer ...\n",
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:    0.2s\n",
            "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:    0.4s\n",
            "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.6s\n",
            "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:    0.7s\n",
            "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:    0.9s\n",
            "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    1.0s\n",
            "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    1.2s\n",
            "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.3s\n",
            "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:    1.4s\n",
            "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:    1.5s\n",
            "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:    1.6s\n",
            "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:    1.7s\n",
            "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:    1.9s\n",
            "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:    2.0s\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.2s\n",
            "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:    2.3s\n",
            "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:    2.5s\n",
            "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:    2.6s\n",
            "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:    2.7s\n",
            "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:    2.8s\n",
            "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    2.9s\n",
            "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
            "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:    3.3s\n",
            "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:    3.5s\n",
            "[Parallel(n_jobs=-1)]: Done  28 out of  30 | elapsed:    3.6s remaining:    0.3s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    4.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    4.0s finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wAZgb9niIK9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "79ccfb9a-8479-47f6-cc4b-b0626be942b2"
      },
      "source": [
        "# Seleção do melhor classificador/vetorizador\n",
        "def get_f1(estimator):\n",
        "  return estimator.get('f1')\n",
        "\n",
        "estimators.sort(key=get_f1, reverse=True)\n",
        "\n",
        "for estimator in estimators:\n",
        "  print(estimator['est_name'],'-',estimator['vec_name'],'- F1:',estimator['f1']*100)\n",
        "\n",
        "# escolhe melhor classificador/vetorizador\n",
        "clf = estimators[0]['est']\n",
        "vectorizer = estimators[0]['vec']\n",
        "print('\\nSelecionado: ',estimators[0]['est_name'],'-',estimators[0]['vec_name'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NearestCentroid - TfidfVectorizer - F1: 48.672078055347576\n",
            "NearestCentroid - CountVectorizer - F1: 44.57278593508021\n",
            "\n",
            "Selecionado:  NearestCentroid - TfidfVectorizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR0iwP-RiA4m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# escolhe manualmente classificador/vetorizador\n",
        "#index = 1\n",
        "#clf = estimators[index]['est']\n",
        "#vectorizer = estimators[index]['vec']\n",
        "#print('\\nSelecionado: ',estimators[index]['est_name'],'-',estimators[index]['vec_name'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0El61_r99b9B",
        "colab_type": "text"
      },
      "source": [
        "## Classificando os registros não rotulados para o desafio\n",
        "\n",
        "Repita os passos acima testando outras preparações de dados, outros vetorizadores, outros parâmetros e outras técnicas de classificação. \n",
        "\n",
        "Quando estiver satisfeito com a performance do seu classificador, deve treiná-lo agora com **todos** os registros pré-rotulados. \n",
        "\n",
        "Esse classificador será então utilizado para inferir as classes dos registros não rotulados do desafio, como veremos a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0LAY1tC4M3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e232fa4-5473-44c1-9b4c-2761a1b06d7a"
      },
      "source": [
        "# Treina o classificador com toda a base fornecida\n",
        "X_train = vectorizer.transform(df['texto'].tolist()).toarray()\n",
        "y_train =  df['classe'].tolist()\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NearestCentroid(metric='euclidean', shrink_threshold=0.1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsOdUsqv92M7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a099689-539e-43d8-9dfe-2ff812e3d401"
      },
      "source": [
        "# Carrega os dados da base não rotulada\n",
        "path = path_meu_google_drive + nome_arquivo_sem_rotulos\n",
        "df_test = pd.read_csv(path, index_col=None, engine='python', sep =',', encoding=\"utf-8\")\n",
        "print('Total de registros carregados:',len(df_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total de registros carregados: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Bt_V5F-KVW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8ae7a17b-a445-4c10-f77d-253029bfdd23"
      },
      "source": [
        "# Prepara os dados para classificação\n",
        "df_test['texto'] = df_test['texto'].apply(limpar_texto)\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>270</td>\n",
              "      <td>leopardo faminto dar mal atacar porco espinho ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>523</td>\n",
              "      <td>general dissidente sair ileso atentado suicida...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>rezar mim pedir francisco aniversario papar pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1294</td>\n",
              "      <td>rottweiler participar trabalho demolicao predi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1349</td>\n",
              "      <td>golpe bilhete premiar levar quatro cadeia pr a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                              texto\n",
              "0   270  leopardo faminto dar mal atacar porco espinho ...\n",
              "1   523  general dissidente sair ileso atentado suicida...\n",
              "2   155  rezar mim pedir francisco aniversario papar pa...\n",
              "3  1294  rottweiler participar trabalho demolicao predi...\n",
              "4  1349  golpe bilhete premiar levar quatro cadeia pr a..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROZIOlLcbK0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salva os registros de treino pre-processados\n",
        "path = path_meu_google_drive + nome_arquivo_sem_rotulos_preprocessado\n",
        "df_test.to_csv(path, index=False, encoding=\"utf-8\", columns=['id','texto'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2ierUOo-YxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vetoriza os textos que serão classificados\n",
        "X_test = vectorizer.transform(df_test['texto'].tolist()).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BvZxeFS-dIz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "b69b2507-b85f-46be-e4db-87dda44ca6f4"
      },
      "source": [
        "# Executa a classificação dos registros não rotulados\n",
        "y_predicted = clf.predict(X_test)\n",
        "df_test['classe'] = y_predicted\n",
        "\n",
        "# Exibe uma amostra dos resultados\n",
        "df_test.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>texto</th>\n",
              "      <th>classe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>270</td>\n",
              "      <td>leopardo faminto dar mal atacar porco espinho ...</td>\n",
              "      <td>surpresa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>523</td>\n",
              "      <td>general dissidente sair ileso atentado suicida...</td>\n",
              "      <td>tristeza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>155</td>\n",
              "      <td>rezar mim pedir francisco aniversario papar pa...</td>\n",
              "      <td>surpresa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1294</td>\n",
              "      <td>rottweiler participar trabalho demolicao predi...</td>\n",
              "      <td>surpresa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1349</td>\n",
              "      <td>golpe bilhete premiar levar quatro cadeia pr a...</td>\n",
              "      <td>desgosto</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>236</td>\n",
              "      <td>usp unicamp unesp adir inicio aulas causa nova...</td>\n",
              "      <td>medo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1691</td>\n",
              "      <td>washington post registrar perdas us dezenove c...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>438</td>\n",
              "      <td>parlamento ucraniano aprovar criacao forca def...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>677</td>\n",
              "      <td>explosoes bancos assustar municipios interior ...</td>\n",
              "      <td>medo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>422</td>\n",
              "      <td>governo china prometer pulso firme contra corr...</td>\n",
              "      <td>neutro</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id                                              texto    classe\n",
              "0   270  leopardo faminto dar mal atacar porco espinho ...  surpresa\n",
              "1   523  general dissidente sair ileso atentado suicida...  tristeza\n",
              "2   155  rezar mim pedir francisco aniversario papar pa...  surpresa\n",
              "3  1294  rottweiler participar trabalho demolicao predi...  surpresa\n",
              "4  1349  golpe bilhete premiar levar quatro cadeia pr a...  desgosto\n",
              "5   236  usp unicamp unesp adir inicio aulas causa nova...      medo\n",
              "6  1691  washington post registrar perdas us dezenove c...    neutro\n",
              "7   438  parlamento ucraniano aprovar criacao forca def...    neutro\n",
              "8   677  explosoes bancos assustar municipios interior ...      medo\n",
              "9   422  governo china prometer pulso firme contra corr...    neutro"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWgeQn8e-qTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salva os registros classificados\n",
        "path = path_meu_google_drive + nome_arquivo_rotulado_classificador\n",
        "df_test.to_csv(path, index=False, encoding=\"utf-8\", columns=['id','classe'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
